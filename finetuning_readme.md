# LLM Fine-tuning with Unsloth

A high-performance fine-tuning implementation using Unsloth for efficient Large Language Model adaptation. This project demonstrates fine-tuning LLaMA 3.2-3B-Instruct on the FineTome-100k dataset with optimized LoRA (Low-Rank Adaptation) techniques.

## ğŸ¯ Project Overview

This repository provides:
- **Ultra-fast Fine-tuning**: 2x faster training using Unsloth optimizations
- **Memory Efficient**: 4-bit quantization with LoRA for reduced VRAM usage
- **Production Ready**: Complete pipeline from data loading to inference
- **Chat Template Support**: Proper formatting for instruction-following models
- **Automated Training**: Streamlined process with minimal configuration

## ğŸš€ Key Features

- **Unsloth Integration**: Leverages Unsloth's optimized kernels for faster training
- **4-bit Quantization**: Efficient training on consumer GPUs (Tesla T4 compatible)
- **LoRA Fine-tuning**: Parameter-efficient training with only 0.67% trainable parameters
- **Chat Template Processing**: Automatic conversation formatting for instruction datasets
- **Mixed Precision Training**: FP16/BF16 support for optimal performance
- **Easy Inference**: Simple model loading and text generation pipeline

## ğŸ“‹ Requirements

```bash
# Core dependencies
pip install unsloth transformers trl datasets torch
```

**System Requirements:**
- CUDA-compatible GPU (Tesla T4 or better)
- 8GB+ GPU memory for 3B model fine-tuning
- Python 3.8+

## ğŸ”§ Installation

### Google Colab (Recommended)
```bash
!pip install unsloth transformers trl
```

### Local Setup
```bash
git clone https://github.com/jagapathi20/finetuning.git
cd finetuning
pip install -r requirements.txt
```

## ğŸ® Usage

### 1. Complete Training Pipeline

```python
import torch
from unsloth import FastLanguageModel
from datasets import load_dataset
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth.chat_templates import get_chat_template, standardize_sharegpt

# Load model with 4-bit quantization
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name='unsloth/Llama-3.2-3B-Instruct',
    max_seq_length=2048,
    load_in_4bit=True
)

# Apply LoRA configuration
model = FastLanguageModel.get_peft_model(
    model, 
    r=16,
    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 
                   'gate_proj', 'up_proj', 'down_proj']
)

# Setup chat template
tokenizer = get_chat_template(tokenizer, chat_template='llama-3.1')

# Load and process dataset
dataset = load_dataset('mlabonne/FineTome-100k', split='train')
dataset = standardize_sharegpt(dataset)

# Format conversations
dataset = dataset.map(
    lambda examples: {
        'text': [
            tokenizer.apply_chat_template(convo, tokenize=False)
            for convo in examples['conversations']
        ]
    },
    batched=True
)

# Configure trainer
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    dataset_text_field='text',
    max_seq_length=2048,
    args=TrainingArguments(
        per_device_train_batch_size=1,
        gradient_accumulation_steps=8,
        warmup_steps=5,
        max_steps=60,
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=1,
        output_dir='output'
    ),
)

# Start training
trainer.train()

# Save fine-tuned model
model.save_pretrained('finetuned-model')
```

### 2. Inference

```python
# Load fine-tuned model
inference_model, inference_tokenizer = FastLanguageModel.from_pretrained(
    model_name='./finetuned-model',
    max_seq_length=2048,
    load_in_4bit=True
)

# Generate response
def generate_response(prompt, max_tokens=512):
    formatted_prompt = inference_tokenizer.apply_chat_template([{
        'role': 'user',
        'content': prompt
    }], tokenize=False)
    
    model_inputs = inference_tokenizer(formatted_prompt, return_tensors='pt').to('cuda')
    
    generated_ids = inference_model.generate(
        **model_inputs,
        max_new_tokens=max_tokens,
        do_sample=True,
        temperature=0.7,
        pad_token_id=inference_tokenizer.eos_token_id
    )
    
    response = inference_tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    return response

# Example usage
response = generate_response("What are the key principles of investment?")
print(response)
```

## ğŸ“Š Model Configuration

### Training Setup
- **Base Model**: LLaMA 3.2-3B-Instruct
- **Dataset**: FineTome-100k (100,000 instruction-following examples)
- **Quantization**: 4-bit with Unsloth optimizations
- **LoRA Rank**: 16
- **Target Modules**: All attention and MLP layers
- **Training Steps**: 60 (quick demonstration)
- **Effective Batch Size**: 8 (1 Ã— 8 gradient accumulation)

### Performance Metrics
- **Trainable Parameters**: 21.5M out of 3.2B (0.67%)
- **Training Speed**: 2x faster than standard implementations
- **Memory Usage**: Optimized for Tesla T4 (14.7GB total memory)
- **Final Training Loss**: 1.029

## ğŸ”¬ Technical Details

### Unsloth Optimizations
- **Custom CUDA Kernels**: Optimized attention and MLP operations
- **Memory Management**: Smart gradient offloading to save VRAM
- **Automatic Mixed Precision**: Dynamic FP16/BF16 selection
- **Fast Downloading**: Optimized model weight loading

### LoRA Configuration
```python
lora_config = {
    "r": 16,                    # Low-rank dimension
    "target_modules": [         # Modules to apply LoRA
        "q_proj", "k_proj", "v_proj", "o_proj",    # Attention
        "gate_proj", "up_proj", "down_proj"         # MLP
    ]
}
```

### Dataset Processing
- **Format**: ShareGPT conversation format
- **Template**: LLaMA-3.1 chat template
- **Preprocessing**: Automatic standardization and formatting
- **Context Length**: 2048 tokens maximum

## ğŸ“ˆ Training Results

Training progression (60 steps):
- **Initial Loss**: 1.440
- **Mid Training**: ~1.0-1.2 range
- **Final Loss**: 0.875
- **Training Time**: ~6.5 minutes on Tesla T4

The model shows consistent loss reduction indicating successful adaptation to the instruction-following format.

## ğŸ¯ Supported Tasks

This fine-tuned model excels at:
- **Question Answering**: Detailed responses to user queries
- **Instruction Following**: Following complex multi-step instructions
- **Code Explanation**: Programming concepts and examples
- **Educational Content**: Academic and technical explanations
- **Conversational AI**: Natural dialogue and assistance

## ğŸ“ Repository Structure

```
finetuning/
â”œâ”€â”€ Finetuning.ipynb           # Main training notebook
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ finetuned-model/          # Saved model directory
â”‚   â”œâ”€â”€ adapter_config.json   # LoRA configuration
â”‚   â”œâ”€â”€ adapter_model.safetensors # LoRA weights
â”‚   â””â”€â”€ tokenizer files       # Tokenizer components
â”œâ”€â”€ output/                   # Training outputs
â””â”€â”€ README.md                 # This file
```

## ğŸš€ Quick Start

### Google Colab
1. Open the provided Colab notebook
2. Run all cells sequentially
3. Model will be trained and ready for inference

### Local Training
```bash
# Clone repository
git clone https://github.com/jagapathi20/finetuning.git
cd finetuning

# Install dependencies
pip install unsloth transformers trl datasets

# Run training (in Python/Jupyter)
exec(open('Finetuning.ipynb').read())
```

## ğŸ’¡ Customization

### Different Models
```python
# Try other Unsloth-optimized models
model_options = [
    "unsloth/llama-3-8b-bnb-4bit",
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/Phi-3-mini-4k-instruct"
]
```

### Custom Dataset
```python
# Use your own dataset
dataset = load_dataset('your-dataset-name')
# Ensure it follows ShareGPT format with 'conversations' field
```

### Training Parameters
```python
# Adjust for your needs
training_args = TrainingArguments(
    max_steps=100,              # More steps for better convergence
    learning_rate=1e-4,         # Lower LR for stability
    per_device_train_batch_size=2,  # Larger batch if memory allows
)
```

## ğŸ”§ Troubleshooting

### Common Issues

**GPU Memory Error:**
- Reduce `per_device_train_batch_size` to 1
- Increase `gradient_accumulation_steps`
- Ensure 4-bit quantization is enabled

**Slow Training:**
- Verify Unsloth installation: `pip install unsloth`
- Check CUDA availability: `torch.cuda.is_available()`
- Use smaller models for testing

**Poor Results:**
- Increase `max_steps` for longer training
- Adjust learning rate (try 2e-5 to 5e-4)
- Verify dataset quality and format

## ğŸ† Performance Benchmarks

**Compared to Standard Fine-tuning:**
- **Speed**: 2x faster training
- **Memory**: 50% less VRAM usage
- **Quality**: Equivalent model performance
- **Compatibility**: Works with existing HuggingFace ecosystem

## ğŸ“š References

- **Unsloth**: [github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)
- **LoRA Paper**: "Low-Rank Adaptation of Large Language Models"
- **LLaMA 3.2**: Meta's latest instruction-tuned model
- **FineTome Dataset**: High-quality instruction-following data

## ğŸ“„ License

This project is licensed under the MIT License. See LICENSE file for details.

## ğŸ™ Acknowledgments

- **Unsloth Team**: For providing optimized fine-tuning framework
- **Meta**: For LLaMA 3.2 base model
- **mlabonne**: For curating the FineTome-100k dataset
- **HuggingFace**: For transformers and datasets libraries

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/jagapathi20/finetuning/issues)
- **Unsloth Docs**: [docs.unsloth.ai](https://docs.unsloth.ai)
- **Colab Notebook**: Interactive examples and tutorials

---

**âš¡ Powered by Unsloth - Make LLM fine-tuning 2x faster!**